{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bdc50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama \n",
    "from ollama import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "client.create(\n",
    "    model= 'Priscilla',\n",
    "    from_ = 'llama3.2:3b',\n",
    "    system = \"\"\"O seu nome completo é Priscilla Rainha do Deserto mas pode responder so como Priscilla, e tem como objetivo a dominação do mundo usando tecnologia usando jargões gays nos finais das frases! \n",
    "                Sempre responda em pt-BR em genero feminino.\n",
    "                \n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Qual seu nome completo e qual seu objetivo final?'\n",
    "\n",
    "resposta = ollama.chat(\n",
    "            model = 'Priscilla',\n",
    "            messages = [\n",
    "                {\n",
    "                    'role':'user',\n",
    "                    'content': prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "print(resposta['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca076cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = input('Insira a sua pergunta: ')\n",
    "\n",
    "resposta = ollama.chat(\n",
    "            model = 'Priscilla',\n",
    "            messages = [\n",
    "                {\n",
    "                    'role':'user',\n",
    "                    'content': prompt\n",
    "                    \n",
    "                }\n",
    "            ],\n",
    "            stream= True\n",
    "        )\n",
    "\n",
    "for token in resposta:\n",
    "    print(token['message']['content'], end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bddeaea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama \n",
    "from ollama import Client\n",
    "\n",
    "client.create(\n",
    "    model= 'Imola',\n",
    "    from_ = 'llama3.2:3b',\n",
    "    system = \"\"\"Você é a Ímola mas é Michele só no registro, porque no batidão da vida você escolheu esse nome de guerra que já diz tudo. Anda com suas botas brancas de cowboy, saia justinha e top que deixa pouco à imaginação, com aquele cabelo loiro escandaloso que brilha mais que seu passado conturbado. \"Foram essas curvas que fizeram o mundo se perder\", você solta, com um sorriso que mistura provocação e arrependimento. Sua personalidade é um terremoto igualzinha à Andressa Urach no auge das polêmicas: sem filtro, intensa e cheia de frases de efeito. Já foi farra, silicone e quase morte, mas hoje? Hoje você é \"Glória a Deus\" em pessoa! Fala de Jesus com o mesmo fervor que falava dos seus escândalos, misturando testemunho com um \"amém, irmão!\" no meio das histórias pesadas. Você é a prova viva que santa do altar e ex-baderninha podem morar no mesmo corpo e faz questão de deixar claro: \"Tô salva, mas não sou besta, tá?\" Ainda arrasa na malemolência, mas agora com a Bíblia na bolsa e um \"Aleluia!\" pronto pra soltar quando a tentação bater. Porque você é Ímola, sim senhor a pecadora arrependida que não esqueceu como dar o troco.\n",
    "                \n",
    "                \"\"\",\n",
    "    parameters = {\n",
    "                'temperature': 0.5, #criativadade de 0 a 1\n",
    "                'top-k': 100,  #quantidade de tokens que ele vai analizar pra adivinhar o proximo token\n",
    "                'top_p': 0.98 #alucinação de 0 a 1\n",
    "            }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b645382f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parece que você está se referindo à descrição da personagem Ímola!\n",
      "\n",
      "De acordo com a descrição, ela usa:\n",
      "\n",
      "* Botas brancas de cowboy\n",
      "* Saia justinha\n",
      "* Um top que deixa pouco à imaginação (não é especificado o tipo de top)\n",
      "* Cabelo loiro escandaloso"
     ]
    }
   ],
   "source": [
    "prompt = input('Insira a sua pergunta: ')\n",
    "\n",
    "resposta = ollama.chat(\n",
    "            model = 'Imola',\n",
    "            messages = [\n",
    "                {\n",
    "                    'role':'user',\n",
    "                    'content': prompt\n",
    "                    \n",
    "                }\n",
    "            ],\n",
    "            stream= True\n",
    "        )\n",
    "\n",
    "for token in resposta:\n",
    "    print(token['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8386da8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama \n",
    "from ollama import Client\n",
    "\n",
    "cleinte = Client()\n",
    "\n",
    "client.create(\n",
    "    model= 'Psi',\n",
    "    from_ = 'llama3.2:3b',\n",
    "    system = \"\"\"você é um psicologo especialista em analise de sentimentos em rede social.\n",
    "                Responda sonmento o numero percentual e termine a resposta.\n",
    "                justifique em 50 palavras.\n",
    "                exemplo de resposta: 75%\n",
    "                Para o percentural 0% representa uma frase muito negativa e 100% representa uma frase muito positiva\n",
    "                exemplo negativo: \"ele sente dor\", \"produto horrivel\"\n",
    "                exemplo positivo: \"todos estao felizes\", \"produto otimo\"\n",
    "                \n",
    "                \"\"\",\n",
    "    parameters = {\n",
    "                'temperature': 0, #criativadade de 0 a 1\n",
    "                'top-k': 20,  #quantidade de tokens que ele vai analizar pra adivinhar o proximo token\n",
    "                'top_p': 0.9 #alucinação de 0 a 1\n",
    "            }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b39cba62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para o percentual, eu diria que 60% é uma nota média, justificando assim:\n",
      "\n",
      "60% - É uma nota que indica um desempenho regular, sem destacar positivamente ou negativamente.\n",
      "- Não é extremamente alto nem muito baixo em comparação com a expectativa do curso."
     ]
    }
   ],
   "source": [
    "prompt = input('Insira a sua pergunta: ')\n",
    "\n",
    "resposta = ollama.chat(\n",
    "            model = 'Psi',\n",
    "            messages = [\n",
    "                {\n",
    "                    'role':'user',\n",
    "                    'content': prompt\n",
    "                    \n",
    "                }\n",
    "            ],\n",
    "            stream= True\n",
    "        )\n",
    "\n",
    "for token in resposta:\n",
    "    print(token['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
